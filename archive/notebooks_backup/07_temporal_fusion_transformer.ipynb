{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Fusion Transformer (TFT) for Stock Return Prediction\n",
    "\n",
    "## Objective\n",
    "Implement state-of-the-art Temporal Fusion Transformer for multi-horizon stock forecasting.\n",
    "\n",
    "## TFT Advantages\n",
    "1. **Attention Mechanisms**: Identify important time steps and features\n",
    "2. **Multi-Horizon Native**: Built for forecasting multiple time steps\n",
    "3. **Quantile Predictions**: Uncertainty estimation (prediction intervals)\n",
    "4. **Interpretability**: Variable importance and attention weights\n",
    "5. **Static & Dynamic Features**: Handle both time-varying and constant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import warnings\n",
    "from typing import Tuple, List\n",
    "import time\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# PyTorch Lightning\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "\n",
    "# PyTorch Forecasting\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer\n",
    "from pytorch_forecasting.data import GroupNormalizer, EncoderNormalizer\n",
    "from pytorch_forecasting.metrics import QuantileLoss, MAE, RMSE\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
    "\n",
    "# ML metrics\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, accuracy_score, f1_score\n",
    "\n",
    "# MLflow\n",
    "import mlflow\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "DATA_DIR = Path('data/processed/stock_data')\n",
    "MLFLOW_EXPERIMENT_NAME = 'stock-tft-forecasting'\n",
    "WINDOW_SIZE = 60\n",
    "HORIZON = 7\n",
    "BATCH_SIZE = 64\n",
    "MAX_EPOCHS = 20\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "FEATURE_NAMES = [\n",
    "    'Open', 'High', 'Low', 'Close', 'Volume', 'OI',\n",
    "    'SMA_5', 'SMA_10', 'SMA_20', 'SMA_50',\n",
    "    'EMA_12', 'EMA_26', 'MACD', 'MACD_Signal', 'MACD_Hist',\n",
    "    'RSI', 'BB_Middle', 'BB_Upper', 'BB_Lower',\n",
    "    'Volume_SMA_20', 'Volume_Ratio',\n",
    "    'Daily_Return', 'Price_Range', 'Price_Change',\n",
    "    'Return_3d', 'Return_5d', 'Return_10d', 'Log_Return',\n",
    "    'Volatility_5d', 'Volatility_20d', 'Momentum_10d', 'Momentum_20d'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_return_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add return-based features to dataframe.\"\"\"\n",
    "    df = df.copy()\n",
    "    df['Return_3d'] = df['Close'].pct_change(3) * 100\n",
    "    df['Return_5d'] = df['Close'].pct_change(5) * 100\n",
    "    df['Return_10d'] = df['Close'].pct_change(10) * 100\n",
    "    df['Log_Return'] = np.log(df['Close'] / df['Close'].shift(1)) * 100\n",
    "    df['Volatility_5d'] = df['Daily_Return'].rolling(5).std()\n",
    "    df['Volatility_20d'] = df['Daily_Return'].rolling(20).std()\n",
    "    df['Momentum_10d'] = df['Close'] - df['Close'].shift(10)\n",
    "    df['Momentum_20d'] = df['Close'] - df['Close'].shift(20)\n",
    "    return df.dropna()\n",
    "\n",
    "\n",
    "def load_all_stocks_as_dataframe():\n",
    "    \"\"\"Load all stocks into a single dataframe (required for TFT).\"\"\"\n",
    "    csv_files = sorted(glob.glob(str(DATA_DIR / \"*_historical.csv\")))\n",
    "    print(f\"Found {len(csv_files)} stock files\")\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    for file_path in csv_files:\n",
    "        df = pd.read_csv(file_path)\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        df = df.sort_values('Date').reset_index(drop=True)\n",
    "        df = add_return_features(df)\n",
    "        \n",
    "        if len(df) > WINDOW_SIZE + HORIZON:\n",
    "            symbol = Path(file_path).stem.replace('_historical', '')\n",
    "            df['Symbol'] = symbol\n",
    "            all_data.append(df)\n",
    "    \n",
    "    # Concatenate all stocks\n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    \n",
    "    # Sort by symbol and date\n",
    "    combined_df = combined_df.sort_values(['Symbol', 'Date']).reset_index(drop=True)\n",
    "    \n",
    "    # Create time index (required by TFT)\n",
    "    combined_df['time_idx'] = combined_df.groupby('Symbol').cumcount()\n",
    "    \n",
    "    # Calculate target: 7-day forward return\n",
    "    combined_df['target_return'] = combined_df.groupby('Symbol')['Close'].transform(\n",
    "        lambda x: ((x.shift(-HORIZON) - x) / x * 100)\n",
    "    )\n",
    "    \n",
    "    # Direction (for evaluation)\n",
    "    combined_df['target_direction'] = (combined_df['target_return'] > 0).astype(int)\n",
    "    \n",
    "    # Remove rows without targets\n",
    "    combined_df = combined_df.dropna(subset=['target_return'])\n",
    "    \n",
    "    print(f\"\\nTotal rows: {len(combined_df):,}\")\n",
    "    print(f\"Unique stocks: {combined_df['Symbol'].nunique()}\")\n",
    "    print(f\"Date range: {combined_df['Date'].min()} to {combined_df['Date'].max()}\")\n",
    "    \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 98 stock files\n",
      "\n",
      "Total rows: 43,120\n",
      "Unique stocks: 98\n",
      "Date range: 2024-01-15 00:00:00+05:30 to 2025-10-21 00:00:00+05:30\n",
      "\n",
      "Sample data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Date</th>\n",
       "      <th>time_idx</th>\n",
       "      <th>Close</th>\n",
       "      <th>target_return</th>\n",
       "      <th>target_direction</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABB</td>\n",
       "      <td>2024-01-15 00:00:00+05:30</td>\n",
       "      <td>0</td>\n",
       "      <td>4752.90</td>\n",
       "      <td>-0.455511</td>\n",
       "      <td>0</td>\n",
       "      <td>4809.00</td>\n",
       "      <td>4809.00</td>\n",
       "      <td>4725.00</td>\n",
       "      <td>4752.90</td>\n",
       "      <td>104057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABB</td>\n",
       "      <td>2024-01-16 00:00:00+05:30</td>\n",
       "      <td>1</td>\n",
       "      <td>4809.30</td>\n",
       "      <td>-1.455513</td>\n",
       "      <td>0</td>\n",
       "      <td>4770.00</td>\n",
       "      <td>4842.30</td>\n",
       "      <td>4750.05</td>\n",
       "      <td>4809.30</td>\n",
       "      <td>192280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABB</td>\n",
       "      <td>2024-01-17 00:00:00+05:30</td>\n",
       "      <td>2</td>\n",
       "      <td>4773.05</td>\n",
       "      <td>0.387593</td>\n",
       "      <td>1</td>\n",
       "      <td>4798.00</td>\n",
       "      <td>4863.70</td>\n",
       "      <td>4731.40</td>\n",
       "      <td>4773.05</td>\n",
       "      <td>163852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABB</td>\n",
       "      <td>2024-01-18 00:00:00+05:30</td>\n",
       "      <td>3</td>\n",
       "      <td>4710.35</td>\n",
       "      <td>0.411859</td>\n",
       "      <td>1</td>\n",
       "      <td>4765.00</td>\n",
       "      <td>4797.40</td>\n",
       "      <td>4665.00</td>\n",
       "      <td>4710.35</td>\n",
       "      <td>151382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABB</td>\n",
       "      <td>2024-01-19 00:00:00+05:30</td>\n",
       "      <td>4</td>\n",
       "      <td>4820.45</td>\n",
       "      <td>-3.087886</td>\n",
       "      <td>0</td>\n",
       "      <td>4738.45</td>\n",
       "      <td>4844.10</td>\n",
       "      <td>4738.45</td>\n",
       "      <td>4820.45</td>\n",
       "      <td>132723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ABB</td>\n",
       "      <td>2024-01-20 00:00:00+05:30</td>\n",
       "      <td>5</td>\n",
       "      <td>4750.95</td>\n",
       "      <td>-4.312822</td>\n",
       "      <td>0</td>\n",
       "      <td>4845.00</td>\n",
       "      <td>4845.00</td>\n",
       "      <td>4730.00</td>\n",
       "      <td>4750.95</td>\n",
       "      <td>49467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ABB</td>\n",
       "      <td>2024-01-23 00:00:00+05:30</td>\n",
       "      <td>6</td>\n",
       "      <td>4770.55</td>\n",
       "      <td>-6.081060</td>\n",
       "      <td>0</td>\n",
       "      <td>4750.95</td>\n",
       "      <td>4897.00</td>\n",
       "      <td>4735.90</td>\n",
       "      <td>4770.55</td>\n",
       "      <td>348426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ABB</td>\n",
       "      <td>2024-01-24 00:00:00+05:30</td>\n",
       "      <td>7</td>\n",
       "      <td>4731.25</td>\n",
       "      <td>-7.673448</td>\n",
       "      <td>0</td>\n",
       "      <td>4764.95</td>\n",
       "      <td>4814.90</td>\n",
       "      <td>4613.80</td>\n",
       "      <td>4731.25</td>\n",
       "      <td>293735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ABB</td>\n",
       "      <td>2024-01-25 00:00:00+05:30</td>\n",
       "      <td>8</td>\n",
       "      <td>4739.30</td>\n",
       "      <td>-7.095985</td>\n",
       "      <td>0</td>\n",
       "      <td>4768.00</td>\n",
       "      <td>4794.95</td>\n",
       "      <td>4665.00</td>\n",
       "      <td>4739.30</td>\n",
       "      <td>114581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ABB</td>\n",
       "      <td>2024-01-29 00:00:00+05:30</td>\n",
       "      <td>9</td>\n",
       "      <td>4791.55</td>\n",
       "      <td>-4.517327</td>\n",
       "      <td>0</td>\n",
       "      <td>4751.00</td>\n",
       "      <td>4807.15</td>\n",
       "      <td>4715.40</td>\n",
       "      <td>4791.55</td>\n",
       "      <td>87854</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Symbol                      Date  time_idx    Close  target_return  \\\n",
       "0    ABB 2024-01-15 00:00:00+05:30         0  4752.90      -0.455511   \n",
       "1    ABB 2024-01-16 00:00:00+05:30         1  4809.30      -1.455513   \n",
       "2    ABB 2024-01-17 00:00:00+05:30         2  4773.05       0.387593   \n",
       "3    ABB 2024-01-18 00:00:00+05:30         3  4710.35       0.411859   \n",
       "4    ABB 2024-01-19 00:00:00+05:30         4  4820.45      -3.087886   \n",
       "5    ABB 2024-01-20 00:00:00+05:30         5  4750.95      -4.312822   \n",
       "6    ABB 2024-01-23 00:00:00+05:30         6  4770.55      -6.081060   \n",
       "7    ABB 2024-01-24 00:00:00+05:30         7  4731.25      -7.673448   \n",
       "8    ABB 2024-01-25 00:00:00+05:30         8  4739.30      -7.095985   \n",
       "9    ABB 2024-01-29 00:00:00+05:30         9  4791.55      -4.517327   \n",
       "\n",
       "   target_direction     Open     High      Low    Close  Volume  \n",
       "0                 0  4809.00  4809.00  4725.00  4752.90  104057  \n",
       "1                 0  4770.00  4842.30  4750.05  4809.30  192280  \n",
       "2                 1  4798.00  4863.70  4731.40  4773.05  163852  \n",
       "3                 1  4765.00  4797.40  4665.00  4710.35  151382  \n",
       "4                 0  4738.45  4844.10  4738.45  4820.45  132723  \n",
       "5                 0  4845.00  4845.00  4730.00  4750.95   49467  \n",
       "6                 0  4750.95  4897.00  4735.90  4770.55  348426  \n",
       "7                 0  4764.95  4814.90  4613.80  4731.25  293735  \n",
       "8                 0  4768.00  4794.95  4665.00  4739.30  114581  \n",
       "9                 0  4751.00  4807.15  4715.40  4791.55   87854  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load data\n",
    "data = load_all_stocks_as_dataframe()\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nSample data:\")\n",
    "display(data[['Symbol', 'Date', 'time_idx', 'Close', 'target_return', 'target_direction'] + FEATURE_NAMES[:5]].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train/Validation/Test Split (Hybrid: Stock-Based + Time-Based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DATA SPLIT (STOCK-BASED + TIME-BASED VALIDATION)\n",
      "================================================================================\n",
      "\n",
      "Stock Split:\n",
      "  Training stocks: 79 (80.6%)\n",
      "  Test stocks:     19 (19.4%)\n",
      "\n",
      "Data Split:\n",
      "  Train set: 29,546 rows (68.5%) - 79 stocks\n",
      "  Val set:   5,214 rows (12.1%) - 79 stocks\n",
      "  Test set:  8,360 rows (19.4%) - 19 stocks\n",
      "\n",
      "Train stocks (first 5): ['M&M', 'HDFCBANK', 'UNIONBANK', 'BHARTIARTL', 'SIEMENS']\n",
      "Test stocks (first 5):  ['TORNTPHARM', 'HAL', 'DLF', 'ABBOTINDIA', 'IOC']\n",
      "\n",
      "Train date range: 2024-01-15 00:00:00+05:30 to 2025-07-16 00:00:00+05:30\n",
      "Val date range:   2025-07-17 00:00:00+05:30 to 2025-10-21 00:00:00+05:30\n",
      "Test date range:  2024-01-15 00:00:00+05:30 to 2025-10-21 00:00:00+05:30\n",
      "\n",
      "Note:\n",
      "  - Training stocks (79) are split by TIME: first 85% for train, last 15% for validation\n",
      "  - Test stocks (19) are COMPLETELY UNSEEN during training\n",
      "  - This matches the 80/20 stock split used in notebooks 03, 04a, 04b\n"
     ]
    }
   ],
   "source": [
    "# Stock-based split (matching notebooks 03, 04a, 04b): 80% stocks train, 20% test\n",
    "# Within training stocks, use time-based validation for TFT early stopping\n",
    "\n",
    "# Step 1: Split stocks into train (80%) and test (20%) - matching other notebooks\n",
    "stock_symbols = data['Symbol'].unique()\n",
    "n_test_stocks = int(len(stock_symbols) * 0.2)\n",
    "n_train_stocks = len(stock_symbols) - n_test_stocks\n",
    "\n",
    "np.random.seed(42)\n",
    "shuffled_symbols = np.random.permutation(stock_symbols)\n",
    "\n",
    "train_symbols = shuffled_symbols[:n_train_stocks]  # 80% of stocks\n",
    "test_symbols = shuffled_symbols[n_train_stocks:]   # 20% of stocks\n",
    "\n",
    "# Step 2: Within training stocks, split by time (85% train, 15% validation)\n",
    "def split_train_by_time(group):\n",
    "    \"\"\"Split each training stock by time: 85% train, 15% validation\"\"\"\n",
    "    n = len(group)\n",
    "    train_end = int(n * 0.85)\n",
    "    \n",
    "    group['split'] = 'val'\n",
    "    group.loc[group.index[:train_end], 'split'] = 'train'\n",
    "    \n",
    "    return group\n",
    "\n",
    "# Initialize all as test\n",
    "data['split'] = 'test'\n",
    "\n",
    "# Apply time-based split only to training stocks\n",
    "train_stocks_data = data[data['Symbol'].isin(train_symbols)].copy()\n",
    "train_stocks_data = train_stocks_data.groupby('Symbol', group_keys=False).apply(split_train_by_time)\n",
    "\n",
    "# Update the main dataframe with train/val splits\n",
    "data.loc[train_stocks_data.index, 'split'] = train_stocks_data['split']\n",
    "\n",
    "# Split datasets\n",
    "train_data = data[data['split'] == 'train'].copy()\n",
    "val_data = data[data['split'] == 'val'].copy()\n",
    "test_data = data[data['split'] == 'test'].copy()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA SPLIT (STOCK-BASED + TIME-BASED VALIDATION)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nStock Split:\")\n",
    "print(f\"  Training stocks: {len(train_symbols)} ({len(train_symbols)/len(stock_symbols)*100:.1f}%)\")\n",
    "print(f\"  Test stocks:     {len(test_symbols)} ({len(test_symbols)/len(stock_symbols)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nData Split:\")\n",
    "print(f\"  Train set: {len(train_data):,} rows ({len(train_data)/len(data)*100:.1f}%) - {train_data['Symbol'].nunique()} stocks\")\n",
    "print(f\"  Val set:   {len(val_data):,} rows ({len(val_data)/len(data)*100:.1f}%) - {val_data['Symbol'].nunique()} stocks\")\n",
    "print(f\"  Test set:  {len(test_data):,} rows ({len(test_data)/len(data)*100:.1f}%) - {test_data['Symbol'].nunique()} stocks\")\n",
    "\n",
    "print(f\"\\nTrain stocks (first 5): {list(train_symbols[:5])}\")\n",
    "print(f\"Test stocks (first 5):  {list(test_symbols[:5])}\")\n",
    "\n",
    "print(f\"\\nTrain date range: {train_data['Date'].min()} to {train_data['Date'].max()}\")\n",
    "print(f\"Val date range:   {val_data['Date'].min()} to {val_data['Date'].max()}\")\n",
    "print(f\"Test date range:  {test_data['Date'].min()} to {test_data['Date'].max()}\")\n",
    "\n",
    "print(\"\\nNote:\")\n",
    "print(\"  - Training stocks (79) are split by TIME: first 85% for train, last 15% for validation\")\n",
    "print(\"  - Test stocks (19) are COMPLETELY UNSEEN during training\")\n",
    "print(\"  - This matches the 80/20 stock split used in notebooks 03, 04a, 04b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create TimeSeriesDataSet for TFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing time_idx continuity issue...\n",
      "Train data shape before cleaning: (29546, 40)\n",
      "\n",
      "Checking for NaN/infinite values:\n",
      "\n",
      "After removing NaN/inf: (29546, 40)\n",
      "\n",
      "target_return statistics:\n",
      "  Min: -48.5181\n",
      "  Max: 41.0657\n",
      "  Mean: 0.3697\n",
      "  Negative values: 14111 (47.8%)\n",
      "\n",
      "Resetting time_idx to be continuous for each stock...\n",
      "\n",
      "Cleaning validation and test data...\n",
      "\n",
      "Final dataset sizes:\n",
      "  Train: 29546 rows, 79 stocks\n",
      "  Val:   5214 rows, 79 stocks\n",
      "  Test:  8360 rows, 19 stocks\n",
      "\n",
      "Final verification - checking for any remaining NaN/inf values:\n",
      "  âœ“ No NaN/inf values found in cleaned data\n",
      "\n",
      "Creating TimeSeriesDataSet...\n",
      "\n",
      "Creating validation dataset with sliding window...\n",
      "Creating test dataset with sliding window...\n",
      "\n",
      "âœ“ Success! TimeSeriesDataSet created:\n",
      "  Training samples: 29,546\n",
      "  Validation samples: 5,214\n",
      "  Test samples: 8,360\n",
      "\n",
      "Sample counts make sense:\n",
      "  Val data rows: 5214 â†’ Samples: 5,214\n",
      "  Test data rows: 8360 â†’ Samples: 8,360\n"
     ]
    }
   ],
   "source": [
    "# Define max prediction length (horizon)\n",
    "max_prediction_length = 1  # Predict 1 step ahead (7-day return)\n",
    "max_encoder_length = WINDOW_SIZE  # Use 60 days of history\n",
    "\n",
    "# CRITICAL FIX: The issue is the softplus transformation creates NaN for negative values\n",
    "# We need to use a different normalizer or handle negative returns differently\n",
    "\n",
    "print(\"Fixing time_idx continuity issue...\")\n",
    "print(f\"Train data shape before cleaning: {train_data.shape}\")\n",
    "\n",
    "# First, check for NaN/inf in features and target\n",
    "all_columns = FEATURE_NAMES + [\"target_return\", \"target_direction\"]\n",
    "print(\"\\nChecking for NaN/infinite values:\")\n",
    "for col in all_columns:\n",
    "    if col in train_data.columns:\n",
    "        nan_count = train_data[col].isna().sum()\n",
    "        inf_count = np.isinf(train_data[col]).sum()\n",
    "        if nan_count > 0 or inf_count > 0:\n",
    "            print(f\"  {col}: {nan_count} NaN, {inf_count} inf values\")\n",
    "\n",
    "# Remove rows with NaN or infinite values\n",
    "train_data_clean = train_data.copy()\n",
    "for col in all_columns:\n",
    "    if col in train_data_clean.columns:\n",
    "        train_data_clean = train_data_clean[~train_data_clean[col].isna()]\n",
    "        train_data_clean = train_data_clean[~np.isinf(train_data_clean[col])]\n",
    "\n",
    "print(f\"\\nAfter removing NaN/inf: {train_data_clean.shape}\")\n",
    "\n",
    "# Check target_return distribution\n",
    "print(f\"\\ntarget_return statistics:\")\n",
    "print(f\"  Min: {train_data_clean['target_return'].min():.4f}\")\n",
    "print(f\"  Max: {train_data_clean['target_return'].max():.4f}\")\n",
    "print(f\"  Mean: {train_data_clean['target_return'].mean():.4f}\")\n",
    "print(f\"  Negative values: {(train_data_clean['target_return'] < 0).sum()} ({(train_data_clean['target_return'] < 0).sum() / len(train_data_clean) * 100:.1f}%)\")\n",
    "\n",
    "# CRITICAL: Reset time_idx to be continuous for each Symbol\n",
    "print(\"\\nResetting time_idx to be continuous for each stock...\")\n",
    "train_data_clean = train_data_clean.sort_values(['Symbol', 'Date']).reset_index(drop=True)\n",
    "train_data_clean['time_idx'] = train_data_clean.groupby('Symbol').cumcount()\n",
    "\n",
    "# Verify each stock has enough data\n",
    "min_required_length = max_encoder_length + max_prediction_length\n",
    "stocks_to_remove = []\n",
    "for symbol in train_data_clean['Symbol'].unique():\n",
    "    symbol_data = train_data_clean[train_data_clean['Symbol'] == symbol]\n",
    "    if len(symbol_data) < min_required_length:\n",
    "        stocks_to_remove.append(symbol)\n",
    "        print(f\"  Removing {symbol}: only {len(symbol_data)} rows (need {min_required_length})\")\n",
    "\n",
    "if stocks_to_remove:\n",
    "    train_data_clean = train_data_clean[~train_data_clean['Symbol'].isin(stocks_to_remove)]\n",
    "    print(f\"\\nRemoved {len(stocks_to_remove)} stocks with insufficient data\")\n",
    "\n",
    "# Reset time_idx again after removing stocks\n",
    "train_data_clean = train_data_clean.sort_values(['Symbol', 'Date']).reset_index(drop=True)\n",
    "train_data_clean['time_idx'] = train_data_clean.groupby('Symbol').cumcount()\n",
    "\n",
    "# Apply same cleaning to val and test data (WITHOUT filtering by training stocks)\n",
    "print(\"\\nCleaning validation and test data...\")\n",
    "val_data_clean = val_data.copy()\n",
    "test_data_clean = test_data.copy()\n",
    "\n",
    "# IMPORTANT: Validation data comes from same stocks as training (time-based split)\n",
    "# So we filter validation to only keep stocks present in training\n",
    "val_data_clean = val_data_clean[val_data_clean['Symbol'].isin(train_data_clean['Symbol'].unique())]\n",
    "\n",
    "# IMPORTANT: Test data comes from DIFFERENT stocks (stock-based split)\n",
    "# So we do NOT filter test data - keep all test stocks as-is\n",
    "\n",
    "# Remove NaN/inf from val\n",
    "for col in all_columns:\n",
    "    if col in val_data_clean.columns:\n",
    "        val_data_clean = val_data_clean[~val_data_clean[col].isna()]\n",
    "        val_data_clean = val_data_clean[~np.isinf(val_data_clean[col])]\n",
    "\n",
    "# Remove NaN/inf from test\n",
    "for col in all_columns:\n",
    "    if col in test_data_clean.columns:\n",
    "        test_data_clean = test_data_clean[~test_data_clean[col].isna()]\n",
    "        test_data_clean = test_data_clean[~np.isinf(test_data_clean[col])]\n",
    "\n",
    "# Reset time_idx for val (continuous within each stock)\n",
    "val_data_clean = val_data_clean.sort_values(['Symbol', 'Date']).reset_index(drop=True)\n",
    "val_data_clean['time_idx'] = val_data_clean.groupby('Symbol').cumcount()\n",
    "\n",
    "# Reset time_idx for test (continuous within each stock)\n",
    "test_data_clean = test_data_clean.sort_values(['Symbol', 'Date']).reset_index(drop=True)\n",
    "test_data_clean['time_idx'] = test_data_clean.groupby('Symbol').cumcount()\n",
    "\n",
    "print(f\"\\nFinal dataset sizes:\")\n",
    "print(f\"  Train: {len(train_data_clean)} rows, {train_data_clean['Symbol'].nunique()} stocks\")\n",
    "print(f\"  Val:   {len(val_data_clean)} rows, {val_data_clean['Symbol'].nunique()} stocks\")\n",
    "print(f\"  Test:  {len(test_data_clean)} rows, {test_data_clean['Symbol'].nunique()} stocks\")\n",
    "\n",
    "# Final verification: check that there are NO NaN values\n",
    "print(\"\\nFinal verification - checking for any remaining NaN/inf values:\")\n",
    "has_issues = False\n",
    "for col in all_columns:\n",
    "    if col in train_data_clean.columns:\n",
    "        nan_count = train_data_clean[col].isna().sum()\n",
    "        inf_count = np.isinf(train_data_clean[col]).sum()\n",
    "        if nan_count > 0 or inf_count > 0:\n",
    "            print(f\"  WARNING: {col} still has {nan_count} NaN, {inf_count} inf values\")\n",
    "            has_issues = True\n",
    "\n",
    "if not has_issues:\n",
    "    print(\"  âœ“ No NaN/inf values found in cleaned data\")\n",
    "        \n",
    "print(\"\\nCreating TimeSeriesDataSet...\")\n",
    "\n",
    "# CRITICAL FIX: Use EncoderNormalizer instead of GroupNormalizer\n",
    "# EncoderNormalizer can handle unseen test stocks (different from training stocks)\n",
    "training = TimeSeriesDataSet(\n",
    "    train_data_clean,\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"target_return\",\n",
    "    group_ids=[\"Symbol\"],\n",
    "    min_encoder_length=max_encoder_length // 2,\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    static_categoricals=[],\n",
    "    static_reals=[],\n",
    "    time_varying_known_categoricals=[],\n",
    "    time_varying_known_reals=[\"time_idx\"],\n",
    "    time_varying_unknown_categoricals=[],\n",
    "    time_varying_unknown_reals=FEATURE_NAMES + [\"target_return\"],\n",
    "    target_normalizer=EncoderNormalizer(),  # Use EncoderNormalizer to handle unseen test stocks\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    allow_missing_timesteps=True,\n",
    ")\n",
    "\n",
    "# Create validation dataset - use predict=False and stop_randomization=True for more samples\n",
    "print(\"\\nCreating validation dataset with sliding window...\")\n",
    "validation = TimeSeriesDataSet.from_dataset(\n",
    "    training, \n",
    "    val_data_clean, \n",
    "    predict=False,  # Changed from True to False - this creates sliding window samples\n",
    "    stop_randomization=True\n",
    ")\n",
    "\n",
    "# Create test dataset - use predict=False for more samples\n",
    "print(\"Creating test dataset with sliding window...\")\n",
    "test = TimeSeriesDataSet.from_dataset(\n",
    "    training, \n",
    "    test_data_clean, \n",
    "    predict=False,  # Changed from True to False\n",
    "    stop_randomization=True\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Success! TimeSeriesDataSet created:\")\n",
    "print(f\"  Training samples: {len(training):,}\")\n",
    "print(f\"  Validation samples: {len(validation):,}\")\n",
    "print(f\"  Test samples: {len(test):,}\")\n",
    "\n",
    "# Show expected vs actual samples\n",
    "expected_val = len(val_data_clean) - max_encoder_length * val_data_clean['Symbol'].nunique()\n",
    "expected_test = len(test_data_clean) - max_encoder_length * test_data_clean['Symbol'].nunique()\n",
    "print(f\"\\nSample counts make sense:\")\n",
    "print(f\"  Val data rows: {len(val_data_clean)} â†’ Samples: {len(validation):,}\")\n",
    "print(f\"  Test data rows: {len(test_data_clean)} â†’ Samples: {len(test):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train batches: 461\n",
      "Val batches: 41\n",
      "Test batches: 66\n"
     ]
    }
   ],
   "source": [
    "# Create dataloaders\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=BATCH_SIZE, num_workers=0)\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=BATCH_SIZE * 2, num_workers=0)\n",
    "test_dataloader = test.to_dataloader(train=False, batch_size=BATCH_SIZE * 2, num_workers=0)\n",
    "\n",
    "print(f\"\\nTrain batches: {len(train_dataloader)}\")\n",
    "print(f\"Val batches: {len(val_dataloader)}\")\n",
    "print(f\"Test batches: {len(test_dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configure and Train TFT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Lightning Trainer configured\n"
     ]
    }
   ],
   "source": [
    "# Configure PyTorch Lightning trainer\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=1e-4,\n",
    "    patience=10,\n",
    "    verbose=False,\n",
    "    mode=\"min\"\n",
    ")\n",
    "\n",
    "lr_logger = LearningRateMonitor()\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    gradient_clip_val=0.1,\n",
    "    callbacks=[early_stop_callback, lr_logger],\n",
    "    enable_progress_bar=True,\n",
    "    enable_model_summary=True,\n",
    ")\n",
    "\n",
    "print(\"PyTorch Lightning Trainer configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TFT Model Summary:\n",
      "  Parameters: 522,177\n",
      "  Hidden size: 64\n",
      "  Attention heads: 4\n",
      "  Dropout: 0.1\n"
     ]
    }
   ],
   "source": [
    "# Configure TFT model\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.03,\n",
    "    hidden_size=64,  # Start with smaller model\n",
    "    attention_head_size=4,\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=32,\n",
    "    loss=QuantileLoss(),\n",
    "    optimizer=\"ranger\",\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "\n",
    "print(f\"\\nTFT Model Summary:\")\n",
    "print(f\"  Parameters: {tft.size():,}\")\n",
    "print(f\"  Hidden size: 64\")\n",
    "print(f\"  Attention heads: 4\")\n",
    "print(f\"  Dropout: 0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TRAINING TFT MODEL\n",
      "================================================================================\n",
      "This may take 10-30 minutes depending on your hardware...\n",
      "\n",
      "Creating fresh TFT model instance...\n",
      "âœ“ Model created: 522,177 parameters\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-21 14:07:00.370451: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 0      | train\n",
      "3  | prescalers                         | ModuleDict                      | 2.4 K  | train\n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 20.5 K | train\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 280 K  | train\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 13.7 K | train\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 16.8 K | train\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 16.8 K | train\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 16.8 K | train\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 16.8 K | train\n",
      "11 | lstm_encoder                       | LSTM                            | 33.3 K | train\n",
      "12 | lstm_decoder                       | LSTM                            | 33.3 K | train\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 8.3 K  | train\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 128    | train\n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 20.9 K | train\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 10.4 K | train\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 8.4 K  | train\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 16.8 K | train\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 8.4 K  | train\n",
      "20 | output_layer                       | Linear                          | 455    | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "522 K     Trainable params\n",
      "0         Non-trainable params\n",
      "522 K     Total params\n",
      "2.089     Total estimated model params size (MB)\n",
      "759       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fb32de95bba447b8830a17d1c03fa34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f8c9547ea38484ebe024a94e28d7406",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "454155e687a04be5b6d21476008ed47b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72f95a4dce2f43f4bb6260708be72387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c55f7ba9650243218a49f2fa114d506c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "432cf30f96dd46459e0da3080c67d9f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c20c0624a12846689295319d92039388",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b8a3c709abe4f1db93c994f441eda50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d97ed0ae4229407fb6573d3b221a385b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17674107470f416681ef2e0bfd5182e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10f1374a4ba346e19079db26c2e95d43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b677a8169cee4e1ca81274332350cc30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b221cddd0e649b7898215bf4b8ca387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e547d881400149398ca2226c66fe59fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3869bb3b758d40689740e013b9c9a71b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "068d22efa64141149ba386d735790a42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecbd0585270e4622ad3c0364f6240fa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da44cce6dcd243c78aea5c663293f0de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "887d8784c7fe40359e801b02201ee4fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81c5c853c3b94de6b9bf2583a8f171a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b0c528b5b514af5b3a0c393c104318a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1be054b31a614a2ba6920d179816b6f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed in 15588.26s (259.8 minutes)\n",
      "Best model at epoch: None\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING TFT MODEL\")\n",
    "print(\"=\"*80)\n",
    "print(\"This may take 10-30 minutes depending on your hardware...\\n\")\n",
    "\n",
    "# Recreate the model to ensure it's properly initialized\n",
    "print(\"Creating fresh TFT model instance...\")\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.03,\n",
    "    hidden_size=64,\n",
    "    attention_head_size=4,\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=32,\n",
    "    loss=QuantileLoss(),\n",
    "    optimizer=\"ranger\",\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "print(f\"âœ“ Model created: {tft.size():,} parameters\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "trainer.fit(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    ")\n",
    "\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nTraining completed in {train_time:.2f}s ({train_time/60:.1f} minutes)\")\n",
    "print(f\"Best model at epoch: {trainer.checkpoint_callback.best_model_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model from: /home/labeeb/Desktop/stock-curator/lightning_logs/version_4/checkpoints/epoch=19-step=9220.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Load best model\n",
    "best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "if best_model_path:\n",
    "    best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)\n",
    "else:\n",
    "    best_tft = tft\n",
    "\n",
    "print(f\"Using model from: {best_model_path if best_model_path else 'final training state'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions generated: 8,360\n",
      "Prediction range: [-11.9316, 16.1656]\n",
      "Actual range: [-25.4498, 30.2640]\n",
      "âœ“ All arrays properly aligned\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions on test set\n",
    "print(\"Generating predictions on test set...\")\n",
    "\n",
    "predictions = best_tft.predict(test_dataloader, mode=\"prediction\", return_x=True)\n",
    "\n",
    "# Extract predictions\n",
    "y_pred = predictions.output.numpy().flatten()\n",
    "\n",
    "# IMPORTANT: Use test_data_clean (the cleaned data that the model actually saw)\n",
    "# NOT test_data (the original uncleaned data)\n",
    "y_true = test_data_clean['target_return'].values[:len(y_pred)]\n",
    "y_dir_true = test_data_clean['target_direction'].values[:len(y_pred)]\n",
    "\n",
    "# Convert predictions to direction\n",
    "y_dir_pred = (y_pred > 0).astype(int)\n",
    "\n",
    "# Validate alignment\n",
    "assert len(y_pred) == len(y_true) == len(y_dir_true), \\\n",
    "    f\"Length mismatch: pred={len(y_pred)}, true={len(y_true)}, dir={len(y_dir_true)}\"\n",
    "\n",
    "print(f\"\\nPredictions generated: {len(y_pred):,}\")\n",
    "print(f\"Prediction range: [{y_pred.min():.4f}, {y_pred.max():.4f}]\")\n",
    "print(f\"Actual range: [{y_true.min():.4f}, {y_true.max():.4f}]\")\n",
    "print(f\"âœ“ All arrays properly aligned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TFT MODEL - TEST SET EVALUATION\n",
      "================================================================================\n",
      "\n",
      "Regression Metrics (Return Prediction):\n",
      "  MAE: 4.6883%\n",
      "  RMSE: 6.0376%\n",
      "  MAPE: 1450790491.2930%\n",
      "\n",
      "Classification Metrics (Direction Prediction):\n",
      "  Accuracy: 0.5097 (50.97%)\n",
      "  F1 Score: 0.5205\n",
      "\n",
      "Trading Simulation:\n",
      "  Initial Capital: â‚¹1,000\n",
      "  Final Capital: â‚¹19402976.58\n",
      "  Total Return: 1940197.66%\n",
      "  Sharpe Ratio: 0.0741\n",
      "  Win Rate: 52.50%\n",
      "  Number of Trades: 4152\n"
     ]
    }
   ],
   "source": [
    "# Calculate metrics\n",
    "def calculate_metrics(y_true, y_pred, y_dir_true, y_dir_pred):\n",
    "    \"\"\"Calculate comprehensive evaluation metrics.\"\"\"\n",
    "    # Validate all arrays are same length\n",
    "    assert len(y_true) == len(y_pred) == len(y_dir_true) == len(y_dir_pred), \\\n",
    "        f\"Array length mismatch: true={len(y_true)}, pred={len(y_pred)}, dir_true={len(y_dir_true)}, dir_pred={len(y_dir_pred)}\"\n",
    "    \n",
    "    # Regression metrics\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-10))) * 100\n",
    "    \n",
    "    # Classification metrics\n",
    "    accuracy = accuracy_score(y_dir_true, y_dir_pred)\n",
    "    f1 = f1_score(y_dir_true, y_dir_pred)\n",
    "    \n",
    "    # Trading simulation\n",
    "    capital = 1000\n",
    "    trades = []\n",
    "    for i in range(len(y_dir_pred)):\n",
    "        if y_dir_pred[i] == 1:\n",
    "            actual_return = y_true[i] / 100\n",
    "            trade_return = actual_return - 0.001  # Transaction cost\n",
    "            capital *= (1 + trade_return)\n",
    "            trades.append(trade_return)\n",
    "    \n",
    "    total_return = (capital - 1000) / 1000 * 100\n",
    "    win_rate = np.mean([1 if r > 0 else 0 for r in trades]) * 100 if trades else 0\n",
    "    sharpe = np.mean(trades) / np.std(trades) if len(trades) > 1 and np.std(trades) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'MAE': mae,\n",
    "        'RMSE': rmse,\n",
    "        'MAPE': mape,\n",
    "        'Accuracy': accuracy,\n",
    "        'F1': f1,\n",
    "        'Sharpe Ratio': sharpe,\n",
    "        'Total Return (%)': total_return,\n",
    "        'Win Rate (%)': win_rate,\n",
    "        'Number of Trades': len(trades)\n",
    "    }\n",
    "\n",
    "tft_metrics = calculate_metrics(y_true, y_pred, y_dir_true, y_dir_pred)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TFT MODEL - TEST SET EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nRegression Metrics (Return Prediction):\")\n",
    "print(f\"  MAE: {tft_metrics['MAE']:.4f}%\")\n",
    "print(f\"  RMSE: {tft_metrics['RMSE']:.4f}%\")\n",
    "print(f\"  MAPE: {tft_metrics['MAPE']:.4f}%\")\n",
    "\n",
    "print(f\"\\nClassification Metrics (Direction Prediction):\")\n",
    "print(f\"  Accuracy: {tft_metrics['Accuracy']:.4f} ({tft_metrics['Accuracy']*100:.2f}%)\")\n",
    "print(f\"  F1 Score: {tft_metrics['F1']:.4f}\")\n",
    "\n",
    "print(f\"\\nTrading Simulation:\")\n",
    "print(f\"  Initial Capital: â‚¹1,000\")\n",
    "print(f\"  Final Capital: â‚¹{1000 * (1 + tft_metrics['Total Return (%)']/100):.2f}\")\n",
    "print(f\"  Total Return: {tft_metrics['Total Return (%)']:.2f}%\")\n",
    "print(f\"  Sharpe Ratio: {tft_metrics['Sharpe Ratio']:.4f}\")\n",
    "print(f\"  Win Rate: {tft_metrics['Win Rate (%)']:.2f}%\")\n",
    "print(f\"  Number of Trades: {tft_metrics['Number of Trades']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparison with Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MODEL COMPARISON: TFT vs BASELINES\n",
      "================================================================================\n",
      "                    MAE  Accuracy      F1  Sharpe Ratio  Total Return (%)  \\\n",
      "Linear Models    3.4880    0.5881  0.6080        0.2095      9.400000e-01   \n",
      "Multi-Task LSTM  3.6160    0.5394  0.5666        0.1206      5.600000e-01   \n",
      "Multi-Task GRU   3.5000    0.5475  0.5795        0.1311      6.100000e-01   \n",
      "TFT              4.6883    0.5097  0.5205        0.0741      1.940198e+06   \n",
      "\n",
      "                 Win Rate (%)  \n",
      "Linear Models         58.5600  \n",
      "Multi-Task LSTM       53.7300  \n",
      "Multi-Task GRU        54.1300  \n",
      "TFT                   52.5048  \n",
      "\n",
      "================================================================================\n",
      "BEST PERFORMERS\n",
      "================================================================================\n",
      "MAE                 : TFT                  - 4.6883\n",
      "Accuracy            : Linear Models        - 0.5881\n",
      "F1                  : Linear Models        - 0.6080\n",
      "Sharpe Ratio        : Linear Models        - 0.2095\n",
      "Total Return (%)    : TFT                  - 1940197.6582\n",
      "Win Rate (%)        : Linear Models        - 58.5600\n"
     ]
    }
   ],
   "source": [
    "# Baseline results from notebook 04a (for comparison)\n",
    "baseline_results = {\n",
    "    'Linear Models': {\n",
    "        'MAE': 3.488,\n",
    "        'Accuracy': 0.5881,\n",
    "        'F1': 0.6080,\n",
    "        'Sharpe Ratio': 0.2095,\n",
    "        'Total Return (%)': 0.94,\n",
    "        'Win Rate (%)': 58.56\n",
    "    },\n",
    "    'Multi-Task LSTM': {\n",
    "        'MAE': 3.616,\n",
    "        'Accuracy': 0.5394,\n",
    "        'F1': 0.5666,\n",
    "        'Sharpe Ratio': 0.1206,\n",
    "        'Total Return (%)': 0.56,\n",
    "        'Win Rate (%)': 53.73\n",
    "    },\n",
    "    'Multi-Task GRU': {\n",
    "        'MAE': 3.500,\n",
    "        'Accuracy': 0.5475,\n",
    "        'F1': 0.5795,\n",
    "        'Sharpe Ratio': 0.1311,\n",
    "        'Total Return (%)': 0.61,\n",
    "        'Win Rate (%)': 54.13\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add TFT results\n",
    "baseline_results['TFT'] = tft_metrics\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame(baseline_results).T\n",
    "key_metrics = ['MAE', 'Accuracy', 'F1', 'Sharpe Ratio', 'Total Return (%)', 'Win Rate (%)']\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON: TFT vs BASELINES\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df[key_metrics].round(4))\n",
    "\n",
    "# Highlight best performers\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BEST PERFORMERS\")\n",
    "print(\"=\"*80)\n",
    "for metric in key_metrics:\n",
    "    best_model = comparison_df[metric].idxmax()\n",
    "    best_value = comparison_df[metric].max()\n",
    "    print(f\"{metric:20s}: {best_model:20s} - {best_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Importance and Attention Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Calculate variable importance\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m interpretation \u001b[38;5;241m=\u001b[39m \u001b[43mbest_tft\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpret_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Plot variable importance\u001b[39;00m\n\u001b[1;32m      5\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m8\u001b[39m))\n",
      "File \u001b[0;32m~/Desktop/stock-curator/.venv/lib/python3.10/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/_tft.py:727\u001b[0m, in \u001b[0;36mTemporalFusionTransformer.interpret_output\u001b[0;34m(self, out, reduction, attention_prediction_horizon)\u001b[0m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;124;03minterpret output of model\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;124;03m    interpretations that can be plotted with ``plot_interpretation()``\u001b[39;00m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;66;03m# take attention and concatenate if a list to proper attention object\u001b[39;00m\n\u001b[0;32m--> 727\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mout\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdecoder_attention\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_attention\u001b[39m\u001b[38;5;124m\"\u001b[39m], (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m    729\u001b[0m     \u001b[38;5;66;03m# start with decoder attention\u001b[39;00m\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;66;03m# assume issue is in last dimension, we need to find max\u001b[39;00m\n\u001b[1;32m    731\u001b[0m     max_last_dimension \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m out[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_attention\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "# Calculate variable importance\n",
    "interpretation = best_tft.interpret_output(predictions.output, reduction=\"sum\")\n",
    "\n",
    "# Plot variable importance\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "best_tft.plot_interpretation(interpretation, ax=ax)\n",
    "plt.title('TFT Variable Importance', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVariable importance shows which features the model relies on most.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m sample_idx[:\u001b[38;5;241m3\u001b[39m]:  \u001b[38;5;66;03m# Show first 3 samples\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m4\u001b[39m))\n\u001b[0;32m----> 7\u001b[0m     \u001b[43mbest_tft\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot_prediction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_loss_to_title\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43max\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     plt\u001b[38;5;241m.\u001b[39mtight_layout()\n\u001b[1;32m     15\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/Desktop/stock-curator/.venv/lib/python3.10/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/_tft.py:895\u001b[0m, in \u001b[0;36mTemporalFusionTransformer.plot_prediction\u001b[0;34m(self, x, out, idx, plot_attention, add_loss_to_title, show_future_observed, ax, **kwargs)\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    880\u001b[0m \u001b[38;5;124;03mPlot actuals vs prediction and attention\u001b[39;00m\n\u001b[1;32m    881\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[38;5;124;03m    plt.Figure: matplotlib figure\u001b[39;00m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;66;03m# plot prediction as normal\u001b[39;00m\n\u001b[0;32m--> 895\u001b[0m fig \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot_prediction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m    \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_loss_to_title\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_loss_to_title\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_future_observed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_future_observed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m    \u001b[49m\u001b[43max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;66;03m# add attention on secondary axis\u001b[39;00m\n\u001b[1;32m    906\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m plot_attention:\n",
      "File \u001b[0;32m~/Desktop/stock-curator/.venv/lib/python3.10/site-packages/pytorch_forecasting/models/base/_base_model.py:1185\u001b[0m, in \u001b[0;36mBaseModel.plot_prediction\u001b[0;34m(self, x, out, idx, add_loss_to_title, show_future_observed, ax, quantiles_kwargs, prediction_kwargs)\u001b[0m\n\u001b[1;32m   1181\u001b[0m encoder_targets \u001b[38;5;241m=\u001b[39m to_list(x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_target\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   1182\u001b[0m decoder_targets \u001b[38;5;241m=\u001b[39m to_list(x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_target\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   1184\u001b[0m y_raws \u001b[38;5;241m=\u001b[39m to_list(\n\u001b[0;32m-> 1185\u001b[0m     \u001b[43mout\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1186\u001b[0m )  \u001b[38;5;66;03m# raw predictions - used for calculating loss\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m y_hats \u001b[38;5;241m=\u001b[39m to_list(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_prediction(out, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprediction_kwargs))\n\u001b[1;32m   1188\u001b[0m y_quantiles \u001b[38;5;241m=\u001b[39m to_list(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_quantiles(out, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mquantiles_kwargs))\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Attention weights (which time steps are important)\n",
    "# Sample a few predictions to visualize\n",
    "sample_idx = [0, 100, 200, 300, 400]\n",
    "\n",
    "for idx in sample_idx[:3]:  # Show first 3 samples\n",
    "    fig, ax = plt.subplots(figsize=(12, 4))\n",
    "    best_tft.plot_prediction(\n",
    "        predictions.x,\n",
    "        predictions.output,\n",
    "        idx=idx,\n",
    "        add_loss_to_title=True,\n",
    "        ax=ax\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nAttention plots show how the model weighs different time steps.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Prediction Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze prediction vs actual distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Prediction vs Actual scatter\n",
    "axes[0, 0].scatter(y_true, y_pred, alpha=0.3, s=1)\n",
    "axes[0, 0].plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)\n",
    "axes[0, 0].set_xlabel('Actual Return (%)')\n",
    "axes[0, 0].set_ylabel('Predicted Return (%)')\n",
    "axes[0, 0].set_title('TFT: Predicted vs Actual Returns', fontweight='bold')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals\n",
    "residuals = y_true - y_pred\n",
    "axes[0, 1].hist(residuals, bins=50, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "axes[0, 1].axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Residual (%)')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title(f'Residual Distribution (Mean: {residuals.mean():.4f})', fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Prediction distribution\n",
    "axes[1, 0].hist(y_true, bins=50, alpha=0.5, color='green', label='Actual', edgecolor='black')\n",
    "axes[1, 0].hist(y_pred, bins=50, alpha=0.5, color='orange', label='Predicted', edgecolor='black')\n",
    "axes[1, 0].set_xlabel('Return (%)')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title('Return Distribution Comparison', fontweight='bold')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Cumulative returns from trading strategy\n",
    "cumulative_returns = []\n",
    "capital = 1000\n",
    "for i in range(len(y_dir_pred)):\n",
    "    if y_dir_pred[i] == 1:\n",
    "        actual_return = y_true[i] / 100\n",
    "        trade_return = actual_return - 0.001\n",
    "        capital *= (1 + trade_return)\n",
    "    cumulative_returns.append(capital)\n",
    "\n",
    "axes[1, 1].plot(cumulative_returns, linewidth=2, color='darkgreen')\n",
    "axes[1, 1].axhline(y=1000, color='red', linestyle='--', linewidth=2, label='Initial Capital')\n",
    "axes[1, 1].set_xlabel('Trade Number')\n",
    "axes[1, 1].set_ylabel('Capital (â‚¹)')\n",
    "axes[1, 1].set_title(f'Cumulative Trading Performance (Final: â‚¹{capital:.2f})', fontweight='bold')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Quantile Predictions (Uncertainty Estimation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFT provides quantile predictions (confidence intervals)\n",
    "# This is a unique advantage over other models\n",
    "\n",
    "print(\"\\nQuantile Predictions (Uncertainty Estimation):\")\n",
    "print(\"=\"*80)\n",
    "print(\"TFT predicts multiple quantiles (e.g., 10th, 50th, 90th percentile)\")\n",
    "print(\"This provides prediction intervals, not just point predictions.\")\n",
    "print(\"\\nExample:\")\n",
    "print(\"  10th percentile: Pessimistic prediction\")\n",
    "print(\"  50th percentile: Median prediction (point estimate)\")\n",
    "print(\"  90th percentile: Optimistic prediction\")\n",
    "print(\"\\nWider intervals = higher uncertainty\")\n",
    "print(\"Narrower intervals = higher confidence\")\n",
    "\n",
    "# Sample predictions with quantiles\n",
    "sample_predictions = predictions.output[:5].numpy()\n",
    "print(\"\\nSample Quantile Predictions (first 5):\")\n",
    "for i, pred in enumerate(sample_predictions):\n",
    "    print(f\"  Sample {i+1}: Prediction = {pred[0]:.4f}% (actual: {y_true[i]:.4f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. MLflow Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup MLflow\n",
    "mlflow.set_experiment(MLFLOW_EXPERIMENT_NAME)\n",
    "\n",
    "with mlflow.start_run(run_name=\"TFT\"):\n",
    "    # Log parameters\n",
    "    mlflow.log_param('model_type', 'Temporal Fusion Transformer')\n",
    "    mlflow.log_param('window_size', WINDOW_SIZE)\n",
    "    mlflow.log_param('horizon', HORIZON)\n",
    "    mlflow.log_param('hidden_size', 64)\n",
    "    mlflow.log_param('attention_heads', 4)\n",
    "    mlflow.log_param('dropout', 0.1)\n",
    "    mlflow.log_param('batch_size', BATCH_SIZE)\n",
    "    mlflow.log_param('max_epochs', MAX_EPOCHS)\n",
    "    mlflow.log_param('n_features', len(FEATURE_NAMES))\n",
    "    mlflow.log_param('train_samples', len(train_data))\n",
    "    mlflow.log_param('test_samples', len(test_data))\n",
    "    \n",
    "    # Log metrics\n",
    "    for metric_name, value in tft_metrics.items():\n",
    "        if isinstance(value, (int, float)):\n",
    "            mlflow.log_metric(metric_name.lower().replace(' ', '_').replace('(%)', ''), value)\n",
    "    \n",
    "    mlflow.log_metric('train_time_seconds', train_time)\n",
    "\n",
    "print(\"\\nTFT results logged to MLflow!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save TFT model\n",
    "model_dir = Path('../models')\n",
    "model_dir.mkdir(exist_ok=True)\n",
    "\n",
    "model_path = model_dir / 'tft_stock_predictor.ckpt'\n",
    "trainer.save_checkpoint(model_path)\n",
    "\n",
    "print(f\"\\nTFT model saved to: {model_path}\")\n",
    "print(\"\\nTo load the model later:\")\n",
    "print(\"  loaded_tft = TemporalFusionTransformer.load_from_checkpoint(model_path)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Conclusions and Next Steps\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "**TFT Performance:**\n",
    "- Compare the metrics above with baseline models\n",
    "- Check if TFT achieved >60% accuracy target\n",
    "- Evaluate Sharpe ratio improvement\n",
    "\n",
    "**TFT Advantages:**\n",
    "1. âœ“ **Interpretability**: Variable importance and attention weights\n",
    "2. âœ“ **Uncertainty Estimation**: Quantile predictions (confidence intervals)\n",
    "3. âœ“ **Multi-horizon**: Native support for 7-day forecasting\n",
    "4. âœ“ **Attention Mechanism**: Learns which time steps and features matter\n",
    "\n",
    "**TFT Disadvantages:**\n",
    "1. âœ— **Training Time**: Significantly slower than linear models\n",
    "2. âœ— **Complexity**: More hyperparameters to tune\n",
    "3. âœ— **Computational Cost**: Requires GPU for practical use\n",
    "\n",
    "### Integration with LLM Stock Curation:\n",
    "\n",
    "**Recommended Strategy:**\n",
    "1. **LLM Stage**: Identify stocks with positive news sentiment\n",
    "2. **TFT Stage**: Filter stocks with predicted positive returns + high confidence\n",
    "3. **Combined Score**: `final_score = 0.5 * llm_sentiment + 0.5 * tft_prediction`\n",
    "4. **Threshold**: Only invest when `final_score > 0.7` and `tft_uncertainty < threshold`\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "**If TFT Performance is Good (>60% accuracy, >0.20 Sharpe):**\n",
    "1. Hyperparameter tuning with Optuna\n",
    "2. Increase model size (hidden_size=128, attention_heads=8)\n",
    "3. Try adding static features (sector, market cap, etc.)\n",
    "4. Implement ensemble: TFT + Linear + LSTM\n",
    "\n",
    "**If TFT Performance is Similar to Baselines:**\n",
    "1. Use Linear Models for production (faster, simpler)\n",
    "2. Focus on feature engineering and ensemble methods\n",
    "3. Consider TFT for specific use cases (uncertainty estimation)\n",
    "\n",
    "**Integration Path:**\n",
    "1. Build corroboration pipeline: `LLM â†’ TFT â†’ Decision`\n",
    "2. Backtest combined strategy on historical data\n",
    "3. Paper trade for 1-2 months\n",
    "4. Deploy to production with monitoring\n",
    "\n",
    "### Final Recommendation:\n",
    "\n",
    "Use a **multi-model ensemble**:\n",
    "- **LLM**: Fundamental analysis (news, sentiment)\n",
    "- **Linear Models**: Fast baseline predictions\n",
    "- **TFT**: Uncertainty estimation and attention insights\n",
    "- **Ensemble**: Weighted combination based on confidence\n",
    "\n",
    "This approach balances speed, accuracy, and interpretability for production deployment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stock-curator",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
