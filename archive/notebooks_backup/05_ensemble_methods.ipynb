{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Methods for Stock Return Prediction\n",
    "\n",
    "## Objective\n",
    "Combine multiple models (Linear, LSTM, GRU) to improve prediction accuracy and robustness.\n",
    "\n",
    "## Ensemble Strategies\n",
    "1. **Voting Ensemble**: Majority vote for direction prediction\n",
    "2. **Weighted Average**: Optimize weights based on validation performance\n",
    "3. **Stacking Ensemble**: Meta-learner trained on base model predictions\n",
    "\n",
    "## Expected Improvement\n",
    "- Target Sharpe Ratio: > 0.25 (baseline: 0.21)\n",
    "- Target Accuracy: > 60% (baseline: 58.81%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import warnings\n",
    "from typing import Tuple, Dict, List\n",
    "import time\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Deep learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# MLflow\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import mlflow.keras\n",
    "\n",
    "# Visualization\n",
    "from IPython.display import display\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DATA_DIR = Path('../data/processed/stock_data')\n",
    "MLFLOW_EXPERIMENT_NAME = 'stock-ensemble-methods'\n",
    "WINDOW_SIZE = 60\n",
    "HORIZON = 7\n",
    "TEST_STOCK_RATIO = 0.2\n",
    "\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Window size: {WINDOW_SIZE} days\")\n",
    "print(f\"Forecast horizon: {HORIZON} days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_return_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add return-based features to dataframe.\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Historical returns\n",
    "    df['Return_3d'] = df['Close'].pct_change(3) * 100\n",
    "    df['Return_5d'] = df['Close'].pct_change(5) * 100\n",
    "    df['Return_10d'] = df['Close'].pct_change(10) * 100\n",
    "    \n",
    "    # Log returns\n",
    "    df['Log_Return'] = np.log(df['Close'] / df['Close'].shift(1)) * 100\n",
    "    \n",
    "    # Volatility\n",
    "    df['Volatility_5d'] = df['Daily_Return'].rolling(5).std()\n",
    "    df['Volatility_20d'] = df['Daily_Return'].rolling(20).std()\n",
    "    \n",
    "    # Momentum\n",
    "    df['Momentum_10d'] = df['Close'] - df['Close'].shift(10)\n",
    "    df['Momentum_20d'] = df['Close'] - df['Close'].shift(20)\n",
    "    \n",
    "    return df.dropna()\n",
    "\n",
    "\n",
    "def create_sequences_with_returns(df: pd.DataFrame, window_size: int, horizon: int) -> Tuple:\n",
    "    \"\"\"Create sequences for time series prediction with return targets.\"\"\"\n",
    "    feature_cols = [\n",
    "        'Open', 'High', 'Low', 'Close', 'Volume', 'OI',\n",
    "        'SMA_5', 'SMA_10', 'SMA_20', 'SMA_50',\n",
    "        'EMA_12', 'EMA_26', 'MACD', 'MACD_Signal', 'MACD_Hist',\n",
    "        'RSI', 'BB_Middle', 'BB_Upper', 'BB_Lower',\n",
    "        'Volume_SMA_20', 'Volume_Ratio',\n",
    "        'Daily_Return', 'Price_Range', 'Price_Change',\n",
    "        'Return_3d', 'Return_5d', 'Return_10d', 'Log_Return',\n",
    "        'Volatility_5d', 'Volatility_20d', 'Momentum_10d', 'Momentum_20d'\n",
    "    ]\n",
    "    \n",
    "    X, y_return, y_direction = [], [], []\n",
    "    \n",
    "    for i in range(len(df) - window_size - horizon):\n",
    "        X.append(df[feature_cols].iloc[i:i+window_size].values)\n",
    "        \n",
    "        current_price = df['Close'].iloc[i+window_size-1]\n",
    "        future_price = df['Close'].iloc[i+window_size+horizon-1]\n",
    "        return_pct = ((future_price - current_price) / current_price) * 100\n",
    "        \n",
    "        y_return.append(return_pct)\n",
    "        y_direction.append(1 if return_pct > 0 else 0)\n",
    "    \n",
    "    return np.array(X), np.array(y_return), np.array(y_direction)\n",
    "\n",
    "\n",
    "def load_and_prepare_data():\n",
    "    \"\"\"Load all stock data and prepare train/test sets.\"\"\"\n",
    "    csv_files = sorted(glob.glob(str(DATA_DIR / \"*_historical.csv\")))\n",
    "    print(f\"Found {len(csv_files)} stock files\")\n",
    "    \n",
    "    all_stocks = []\n",
    "    for file_path in csv_files:\n",
    "        df = pd.read_csv(file_path)\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        df = df.sort_values('Date').reset_index(drop=True)\n",
    "        df = add_return_features(df)\n",
    "        \n",
    "        if len(df) > WINDOW_SIZE + HORIZON:\n",
    "            symbol = Path(file_path).stem.replace('_historical', '')\n",
    "            all_stocks.append((symbol, df))\n",
    "    \n",
    "    print(f\"Loaded {len(all_stocks)} stocks with sufficient data\")\n",
    "    \n",
    "    # Split stocks into train/test\n",
    "    np.random.shuffle(all_stocks)\n",
    "    split_idx = int(len(all_stocks) * (1 - TEST_STOCK_RATIO))\n",
    "    train_stocks = all_stocks[:split_idx]\n",
    "    test_stocks = all_stocks[split_idx:]\n",
    "    \n",
    "    print(f\"Training stocks: {len(train_stocks)}\")\n",
    "    print(f\"Testing stocks: {len(test_stocks)}\")\n",
    "    \n",
    "    # Create sequences\n",
    "    X_train, y_return_train, y_dir_train = [], [], []\n",
    "    X_test, y_return_test, y_dir_test = [], [], []\n",
    "    \n",
    "    for symbol, df in train_stocks:\n",
    "        X, y_ret, y_dir = create_sequences_with_returns(df, WINDOW_SIZE, HORIZON)\n",
    "        X_train.append(X)\n",
    "        y_return_train.append(y_ret)\n",
    "        y_dir_train.append(y_dir)\n",
    "    \n",
    "    for symbol, df in test_stocks:\n",
    "        X, y_ret, y_dir = create_sequences_with_returns(df, WINDOW_SIZE, HORIZON)\n",
    "        X_test.append(X)\n",
    "        y_return_test.append(y_ret)\n",
    "        y_dir_test.append(y_dir)\n",
    "    \n",
    "    X_train = np.concatenate(X_train)\n",
    "    y_return_train = np.concatenate(y_return_train)\n",
    "    y_dir_train = np.concatenate(y_dir_train)\n",
    "    \n",
    "    X_test = np.concatenate(X_test)\n",
    "    y_return_test = np.concatenate(y_return_test)\n",
    "    y_dir_test = np.concatenate(y_dir_test)\n",
    "    \n",
    "    print(f\"\\nTrain shape: {X_train.shape}\")\n",
    "    print(f\"Test shape: {X_test.shape}\")\n",
    "    \n",
    "    return X_train, y_return_train, y_dir_train, X_test, y_return_test, y_dir_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "X_train, y_return_train, y_dir_train, X_test, y_return_test, y_dir_test = load_and_prepare_data()\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "n_samples_train, n_timesteps, n_features = X_train.shape\n",
    "n_samples_test = X_test.shape[0]\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train.reshape(-1, n_features)).reshape(n_samples_train, n_timesteps, n_features)\n",
    "X_test_scaled = scaler.transform(X_test.reshape(-1, n_features)).reshape(n_samples_test, n_timesteps, n_features)\n",
    "\n",
    "print(f\"\\nScaled train shape: {X_train_scaled.shape}\")\n",
    "print(f\"Scaled test shape: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train Base Models (Linear, LSTM, GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for linear models (flatten temporal dimension)\n",
    "X_train_flat = X_train_scaled.reshape(n_samples_train, -1)\n",
    "X_test_flat = X_test_scaled.reshape(n_samples_test, -1)\n",
    "\n",
    "print(f\"Flattened train shape: {X_train_flat.shape}\")\n",
    "print(f\"Flattened test shape: {X_test_flat.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Linear Models\n",
    "print(\"Training Linear Models...\")\n",
    "start_time = time.time()\n",
    "\n",
    "linear_reg = LinearRegression()\n",
    "linear_reg.fit(X_train_flat, y_return_train)\n",
    "\n",
    "logistic_clf = LogisticRegression(max_iter=1000, random_state=42)\n",
    "logistic_clf.fit(X_train_flat, y_dir_train)\n",
    "\n",
    "linear_train_time = time.time() - start_time\n",
    "\n",
    "# Predictions\n",
    "linear_return_pred_train = linear_reg.predict(X_train_flat)\n",
    "linear_return_pred_test = linear_reg.predict(X_test_flat)\n",
    "\n",
    "linear_dir_pred_train = logistic_clf.predict(X_train_flat)\n",
    "linear_dir_pred_test = logistic_clf.predict(X_test_flat)\n",
    "linear_dir_proba_test = logistic_clf.predict_proba(X_test_flat)[:, 1]\n",
    "\n",
    "print(f\"Linear Models trained in {linear_train_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LSTM Model\n",
    "print(\"Training LSTM Model...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# LSTM architecture\n",
    "lstm_model = models.Sequential([\n",
    "    layers.Input(shape=(WINDOW_SIZE, n_features)),\n",
    "    layers.LSTM(128, return_sequences=True),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.LSTM(64),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(2, name='output')  # [return, direction]\n",
    "])\n",
    "\n",
    "# Multi-output model\n",
    "return_output = layers.Dense(1, name='return')(lstm_model.layers[-2].output)\n",
    "direction_output = layers.Dense(1, activation='sigmoid', name='direction')(lstm_model.layers[-2].output)\n",
    "\n",
    "lstm_multitask = models.Model(\n",
    "    inputs=lstm_model.input,\n",
    "    outputs=[return_output, direction_output]\n",
    ")\n",
    "\n",
    "lstm_multitask.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss={'return': 'mse', 'direction': 'binary_crossentropy'},\n",
    "    loss_weights={'return': 1.0, 'direction': 1.0},\n",
    "    metrics={'direction': 'accuracy'}\n",
    ")\n",
    "\n",
    "# Train\n",
    "history_lstm = lstm_multitask.fit(\n",
    "    X_train_scaled,\n",
    "    {'return': y_return_train, 'direction': y_dir_train},\n",
    "    validation_split=0.2,\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    verbose=0,\n",
    "    callbacks=[keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]\n",
    ")\n",
    "\n",
    "lstm_train_time = time.time() - start_time\n",
    "\n",
    "# Predictions\n",
    "lstm_return_pred_train, lstm_dir_pred_train = lstm_multitask.predict(X_train_scaled, verbose=0)\n",
    "lstm_return_pred_test, lstm_dir_pred_test = lstm_multitask.predict(X_test_scaled, verbose=0)\n",
    "\n",
    "lstm_return_pred_train = lstm_return_pred_train.flatten()\n",
    "lstm_return_pred_test = lstm_return_pred_test.flatten()\n",
    "lstm_dir_pred_train = (lstm_dir_pred_train.flatten() > 0.5).astype(int)\n",
    "lstm_dir_proba_test = lstm_dir_pred_test.flatten()\n",
    "lstm_dir_pred_test = (lstm_dir_proba_test > 0.5).astype(int)\n",
    "\n",
    "print(f\"LSTM Model trained in {lstm_train_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train GRU Model\n",
    "print(\"Training GRU Model...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# GRU architecture\n",
    "gru_model = models.Sequential([\n",
    "    layers.Input(shape=(WINDOW_SIZE, n_features)),\n",
    "    layers.GRU(128, return_sequences=True),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.GRU(64),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "])\n",
    "\n",
    "# Multi-output model\n",
    "return_output = layers.Dense(1, name='return')(gru_model.layers[-1].output)\n",
    "direction_output = layers.Dense(1, activation='sigmoid', name='direction')(gru_model.layers[-1].output)\n",
    "\n",
    "gru_multitask = models.Model(\n",
    "    inputs=gru_model.input,\n",
    "    outputs=[return_output, direction_output]\n",
    ")\n",
    "\n",
    "gru_multitask.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss={'return': 'mse', 'direction': 'binary_crossentropy'},\n",
    "    loss_weights={'return': 1.0, 'direction': 1.0},\n",
    "    metrics={'direction': 'accuracy'}\n",
    ")\n",
    "\n",
    "# Train\n",
    "history_gru = gru_multitask.fit(\n",
    "    X_train_scaled,\n",
    "    {'return': y_return_train, 'direction': y_dir_train},\n",
    "    validation_split=0.2,\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    verbose=0,\n",
    "    callbacks=[keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]\n",
    ")\n",
    "\n",
    "gru_train_time = time.time() - start_time\n",
    "\n",
    "# Predictions\n",
    "gru_return_pred_train, gru_dir_pred_train = gru_multitask.predict(X_train_scaled, verbose=0)\n",
    "gru_return_pred_test, gru_dir_pred_test = gru_multitask.predict(X_test_scaled, verbose=0)\n",
    "\n",
    "gru_return_pred_train = gru_return_pred_train.flatten()\n",
    "gru_return_pred_test = gru_return_pred_test.flatten()\n",
    "gru_dir_pred_train = (gru_dir_pred_train.flatten() > 0.5).astype(int)\n",
    "gru_dir_proba_test = gru_dir_pred_test.flatten()\n",
    "gru_dir_pred_test = (gru_dir_proba_test > 0.5).astype(int)\n",
    "\n",
    "print(f\"GRU Model trained in {gru_train_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_regression_metrics(y_true, y_pred):\n",
    "    \"\"\"Calculate regression metrics.\"\"\"\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-10))) * 100\n",
    "    return {'MAE': mae, 'RMSE': rmse, 'R2': r2, 'MAPE': mape}\n",
    "\n",
    "\n",
    "def calculate_classification_metrics(y_true, y_pred, y_pred_proba=None):\n",
    "    \"\"\"Calculate classification metrics.\"\"\"\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    auc = roc_auc_score(y_true, y_pred_proba) if y_pred_proba is not None else 0.5\n",
    "    return {'Accuracy': acc, 'Precision': prec, 'Recall': rec, 'F1': f1, 'AUC': auc}\n",
    "\n",
    "\n",
    "def calculate_sharpe_ratio(returns, risk_free_rate=0.0):\n",
    "    \"\"\"Calculate Sharpe ratio.\"\"\"\n",
    "    excess_returns = returns - risk_free_rate\n",
    "    if excess_returns.std() == 0:\n",
    "        return 0.0\n",
    "    return excess_returns.mean() / excess_returns.std()\n",
    "\n",
    "\n",
    "def simulate_trading(y_direction_true, y_direction_pred, y_return_true, position_size=1000, transaction_cost=0.001):\n",
    "    \"\"\"Simulate trading based on predictions.\"\"\"\n",
    "    capital = position_size\n",
    "    trades = []\n",
    "    \n",
    "    for i in range(len(y_direction_pred)):\n",
    "        if y_direction_pred[i] == 1:  # Predict UP\n",
    "            actual_return = y_return_true[i] / 100\n",
    "            trade_return = actual_return - transaction_cost\n",
    "            capital *= (1 + trade_return)\n",
    "            trades.append(trade_return)\n",
    "    \n",
    "    total_return = (capital - position_size) / position_size * 100\n",
    "    win_rate = np.mean([1 if r > 0 else 0 for r in trades]) * 100 if trades else 0\n",
    "    sharpe = calculate_sharpe_ratio(np.array(trades)) if len(trades) > 1 else 0\n",
    "    \n",
    "    return {\n",
    "        'Initial Capital': position_size,\n",
    "        'Final Capital': capital,\n",
    "        'Total Return (%)': total_return,\n",
    "        'Number of Trades': len(trades),\n",
    "        'Win Rate (%)': win_rate,\n",
    "        'Sharpe Ratio': sharpe\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_model(name, y_return_true, y_return_pred, y_dir_true, y_dir_pred, y_dir_proba, train_time):\n",
    "    \"\"\"Comprehensive model evaluation.\"\"\"\n",
    "    reg_metrics = calculate_regression_metrics(y_return_true, y_return_pred)\n",
    "    clf_metrics = calculate_classification_metrics(y_dir_true, y_dir_pred, y_dir_proba)\n",
    "    trading_metrics = simulate_trading(y_dir_true, y_dir_pred, y_return_true)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{name} - Test Set Evaluation\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"\\nRegression Metrics (Return Prediction):\")\n",
    "    for k, v in reg_metrics.items():\n",
    "        print(f\"  {k}: {v:.4f}\")\n",
    "    \n",
    "    print(f\"\\nClassification Metrics (Direction Prediction):\")\n",
    "    for k, v in clf_metrics.items():\n",
    "        print(f\"  {k}: {v:.4f}\")\n",
    "    \n",
    "    print(f\"\\nTrading Simulation:\")\n",
    "    for k, v in trading_metrics.items():\n",
    "        print(f\"  {k}: {v:.4f}\")\n",
    "    \n",
    "    print(f\"\\nTraining Time: {train_time:.2f}s\")\n",
    "    \n",
    "    return {**reg_metrics, **clf_metrics, **trading_metrics, 'Train Time': train_time}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate base models\n",
    "linear_results = evaluate_model(\n",
    "    \"Linear Models\",\n",
    "    y_return_test, linear_return_pred_test,\n",
    "    y_dir_test, linear_dir_pred_test, linear_dir_proba_test,\n",
    "    linear_train_time\n",
    ")\n",
    "\n",
    "lstm_results = evaluate_model(\n",
    "    \"LSTM Multi-Task\",\n",
    "    y_return_test, lstm_return_pred_test,\n",
    "    y_dir_test, lstm_dir_pred_test, lstm_dir_proba_test,\n",
    "    lstm_train_time\n",
    ")\n",
    "\n",
    "gru_results = evaluate_model(\n",
    "    \"GRU Multi-Task\",\n",
    "    y_return_test, gru_return_pred_test,\n",
    "    y_dir_test, gru_dir_pred_test, gru_dir_proba_test,\n",
    "    gru_train_time\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ensemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Voting Ensemble (Majority Vote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple voting ensemble for direction\n",
    "voting_dir_pred_test = ((linear_dir_pred_test + lstm_dir_pred_test + gru_dir_pred_test) >= 2).astype(int)\n",
    "\n",
    "# Average predictions for return\n",
    "voting_return_pred_test = (linear_return_pred_test + lstm_return_pred_test + gru_return_pred_test) / 3\n",
    "\n",
    "# Average probabilities\n",
    "voting_dir_proba_test = (linear_dir_proba_test + lstm_dir_proba_test + gru_dir_proba_test) / 3\n",
    "\n",
    "voting_results = evaluate_model(\n",
    "    \"Voting Ensemble (Unweighted)\",\n",
    "    y_return_test, voting_return_pred_test,\n",
    "    y_dir_test, voting_dir_pred_test, voting_dir_proba_test,\n",
    "    0.0  # No additional training time\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Weighted Average Ensemble (Optimized Weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate weights based on validation performance (Sharpe ratio)\n",
    "sharpe_scores = np.array([\n",
    "    linear_results['Sharpe Ratio'],\n",
    "    lstm_results['Sharpe Ratio'],\n",
    "    gru_results['Sharpe Ratio']\n",
    "])\n",
    "\n",
    "# Normalize to sum to 1 (handle negative Sharpe)\n",
    "sharpe_scores = np.maximum(sharpe_scores, 0)  # Clip negative values\n",
    "if sharpe_scores.sum() > 0:\n",
    "    weights = sharpe_scores / sharpe_scores.sum()\n",
    "else:\n",
    "    weights = np.array([1/3, 1/3, 1/3])  # Equal weights if all negative\n",
    "\n",
    "print(f\"\\nOptimized Weights (based on Sharpe ratio):\")\n",
    "print(f\"  Linear: {weights[0]:.4f}\")\n",
    "print(f\"  LSTM: {weights[1]:.4f}\")\n",
    "print(f\"  GRU: {weights[2]:.4f}\")\n",
    "\n",
    "# Weighted predictions\n",
    "weighted_return_pred_test = (\n",
    "    weights[0] * linear_return_pred_test +\n",
    "    weights[1] * lstm_return_pred_test +\n",
    "    weights[2] * gru_return_pred_test\n",
    ")\n",
    "\n",
    "weighted_dir_proba_test = (\n",
    "    weights[0] * linear_dir_proba_test +\n",
    "    weights[1] * lstm_dir_proba_test +\n",
    "    weights[2] * gru_dir_proba_test\n",
    ")\n",
    "\n",
    "weighted_dir_pred_test = (weighted_dir_proba_test > 0.5).astype(int)\n",
    "\n",
    "weighted_results = evaluate_model(\n",
    "    \"Weighted Ensemble (Sharpe-based)\",\n",
    "    y_return_test, weighted_return_pred_test,\n",
    "    y_dir_test, weighted_dir_pred_test, weighted_dir_proba_test,\n",
    "    0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Stacking Ensemble (Meta-Learner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create meta-features from base model predictions (using train set)\n",
    "meta_features_train = np.column_stack([\n",
    "    linear_return_pred_train,\n",
    "    lstm_return_pred_train,\n",
    "    gru_return_pred_train,\n",
    "    linear_dir_pred_train,\n",
    "    lstm_dir_pred_train,\n",
    "    gru_dir_pred_train\n",
    "])\n",
    "\n",
    "meta_features_test = np.column_stack([\n",
    "    linear_return_pred_test,\n",
    "    lstm_return_pred_test,\n",
    "    gru_return_pred_test,\n",
    "    linear_dir_pred_test,\n",
    "    lstm_dir_pred_test,\n",
    "    gru_dir_pred_test\n",
    "])\n",
    "\n",
    "print(f\"Meta-features shape: {meta_features_train.shape}\")\n",
    "\n",
    "# Train meta-learners\n",
    "print(\"\\nTraining Stacking Meta-Learners...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Return meta-learner (Random Forest)\n",
    "meta_return_model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)\n",
    "meta_return_model.fit(meta_features_train, y_return_train)\n",
    "\n",
    "# Direction meta-learner (Random Forest)\n",
    "meta_dir_model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)\n",
    "meta_dir_model.fit(meta_features_train, y_dir_train)\n",
    "\n",
    "stacking_train_time = time.time() - start_time\n",
    "\n",
    "# Predictions\n",
    "stacking_return_pred_test = meta_return_model.predict(meta_features_test)\n",
    "stacking_dir_pred_test = meta_dir_model.predict(meta_features_test)\n",
    "stacking_dir_proba_test = meta_dir_model.predict_proba(meta_features_test)[:, 1]\n",
    "\n",
    "stacking_results = evaluate_model(\n",
    "    \"Stacking Ensemble (RF Meta-Learner)\",\n",
    "    y_return_test, stacking_return_pred_test,\n",
    "    y_dir_test, stacking_dir_pred_test, stacking_dir_proba_test,\n",
    "    stacking_train_time\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "results_df = pd.DataFrame([\n",
    "    linear_results,\n",
    "    lstm_results,\n",
    "    gru_results,\n",
    "    voting_results,\n",
    "    weighted_results,\n",
    "    stacking_results\n",
    "], index=[\n",
    "    'Linear Models',\n",
    "    'LSTM Multi-Task',\n",
    "    'GRU Multi-Task',\n",
    "    'Voting Ensemble',\n",
    "    'Weighted Ensemble',\n",
    "    'Stacking Ensemble'\n",
    "])\n",
    "\n",
    "# Select key metrics\n",
    "key_metrics = ['MAE', 'Accuracy', 'F1', 'Sharpe Ratio', 'Total Return (%)', 'Win Rate (%)', 'Train Time']\n",
    "comparison_df = results_df[key_metrics]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ENSEMBLE METHODS - FINAL COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "display(comparison_df.round(4))\n",
    "\n",
    "# Highlight best performers\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BEST PERFORMERS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Best Accuracy: {comparison_df['Accuracy'].idxmax()} - {comparison_df['Accuracy'].max():.4f}\")\n",
    "print(f\"Best Sharpe Ratio: {comparison_df['Sharpe Ratio'].idxmax()} - {comparison_df['Sharpe Ratio'].max():.4f}\")\n",
    "print(f\"Best Total Return: {comparison_df['Total Return (%)'].idxmax()} - {comparison_df['Total Return (%)'].max():.2f}%\")\n",
    "print(f\"Best F1 Score: {comparison_df['F1'].idxmax()} - {comparison_df['F1'].max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Accuracy comparison\n",
    "comparison_df['Accuracy'].plot(kind='barh', ax=axes[0, 0], color='steelblue')\n",
    "axes[0, 0].set_title('Directional Accuracy Comparison', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Accuracy')\n",
    "axes[0, 0].axvline(x=0.5, color='red', linestyle='--', label='Random Baseline')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Sharpe Ratio comparison\n",
    "comparison_df['Sharpe Ratio'].plot(kind='barh', ax=axes[0, 1], color='green')\n",
    "axes[0, 1].set_title('Sharpe Ratio Comparison', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Sharpe Ratio')\n",
    "axes[0, 1].axvline(x=0.2095, color='red', linestyle='--', label='Baseline (Linear)')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Total Return comparison\n",
    "comparison_df['Total Return (%)'].plot(kind='barh', ax=axes[1, 0], color='orange')\n",
    "axes[1, 0].set_title('Total Return Comparison', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Total Return (%)')\n",
    "\n",
    "# MAE comparison\n",
    "comparison_df['MAE'].plot(kind='barh', ax=axes[1, 1], color='coral')\n",
    "axes[1, 1].set_title('Return Prediction MAE Comparison', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('MAE (%)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. MLflow Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup MLflow\n",
    "mlflow.set_experiment(MLFLOW_EXPERIMENT_NAME)\n",
    "\n",
    "# Log ensemble results\n",
    "ensemble_models = [\n",
    "    ('Voting Ensemble', voting_results),\n",
    "    ('Weighted Ensemble', weighted_results),\n",
    "    ('Stacking Ensemble', stacking_results)\n",
    "]\n",
    "\n",
    "for model_name, results in ensemble_models:\n",
    "    with mlflow.start_run(run_name=model_name):\n",
    "        # Log parameters\n",
    "        mlflow.log_param('model_type', model_name)\n",
    "        mlflow.log_param('window_size', WINDOW_SIZE)\n",
    "        mlflow.log_param('horizon', HORIZON)\n",
    "        mlflow.log_param('n_features', n_features)\n",
    "        \n",
    "        # Log metrics\n",
    "        for metric_name, value in results.items():\n",
    "            if isinstance(value, (int, float)):\n",
    "                mlflow.log_metric(metric_name.lower().replace(' ', '_'), value)\n",
    "\n",
    "print(\"\\nAll ensemble results logged to MLflow!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusions\n",
    "\n",
    "### Key Findings:\n",
    "1. **Voting Ensemble**: Simple majority vote for robustness\n",
    "2. **Weighted Ensemble**: Performance-based weighting improves predictions\n",
    "3. **Stacking Ensemble**: Meta-learner can capture complex patterns\n",
    "\n",
    "### Recommendations:\n",
    "- Use the **best ensemble** (highest Sharpe ratio) for production\n",
    "- Combine with LLM sentiment signals for stock curation\n",
    "- Consider retraining weights periodically as market conditions change\n",
    "\n",
    "### Next Steps:\n",
    "- Feature selection to reduce dimensionality\n",
    "- Try Temporal Fusion Transformer for better long-horizon predictions\n",
    "- Integrate with LLM news scraping pipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
